{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROB FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "    \n",
    "def init_weights(m):\n",
    "    if (type(m) == nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST', bs=64, n_instances=5000):\n",
    "    if dataset == 'MNIST':\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 60000 - n_instances])\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        c, h, w = 1, 28, 28\n",
    "        \n",
    "    else: #dataset == 'SVHN'\n",
    "        mat = scipy.io.loadmat('./data/train_32x32.mat')\n",
    "        trainset = torch.utils.data.TensorDataset((torch.Tensor(mat['X']) / 255).permute(3, 2, 0, 1),\n",
    "                                                 torch.Tensor(mat['y']))\n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 73257 - n_instances])\n",
    "        trainloader =  torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        c, h, w = 3, 32, 32\n",
    "        \n",
    "    return trainloader, valloader, (c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEF VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D, M, variant='Normal', partial=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "        \n",
    "        self.M2 = self.M*2 if self.partial else self.M   \n",
    "        \n",
    "        self.enc1 = nn.Linear(self.D, 256)\n",
    "        self.enc2 = nn.Linear(self.enc1.out_features, self.enc1.out_features//2)\n",
    "        self.enc3 = nn.Linear(self.enc2.out_features, self.enc2.out_features//2)\n",
    "        self.enc4 = nn.Linear(self.enc3.out_features, self.M2)\n",
    "\n",
    "        if self.variant != 'Normal':\n",
    "            self.enc1.weight.requires_grad = False\n",
    "            self.enc1.bias.requires_grad = False\n",
    "            self.enc2.weight.requires_grad = False\n",
    "            self.enc2.bias.requires_grad = False\n",
    "            self.enc3.weight.requires_grad = False\n",
    "            self.enc3.bias.requires_grad = False\n",
    "            if not self.partial:\n",
    "                self.enc4.weight.requires_grad = False\n",
    "                self.enc4.bias.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.leaky_relu(self.enc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.enc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.enc3(x), 0.2)\n",
    "        x = self.enc4(x)\n",
    "        return torch.chunk(x, 2, dim=1)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_std):\n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            mu = torch.cat((mu, log_std), dim=1)\n",
    "            std = 0.001\n",
    "            eps = torch.randn_like(mu)\n",
    "            return mu + (eps*std)\n",
    "        else:\n",
    "            std = log_std.exp()\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + (eps*std)\n",
    "    \n",
    "    \n",
    "    def log_prob(self, z, mu, log_std):\n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            mu = torch.cat((mu, log_std), dim=1)\n",
    "            return log_normal_diag(z, mu, (torch.ones(z.shape)*torch.log(torch.tensor(0.001**2))).to(device))\n",
    "        else:\n",
    "            return log_normal_diag(z, mu, log_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, D, M):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.M, 128) \n",
    "        self.dec2 = nn.Linear(self.dec1.out_features, self.dec1.out_features*2)\n",
    "        self.dec3 = nn.Linear(self.dec2.out_features, self.dec2.out_features*2)\n",
    "        self.dec4 = nn.Linear(self.dec3.out_features, self.dec3.out_features*2)        \n",
    "        self.dec5 = nn.Linear(self.dec4.out_features, self.D)\n",
    "    \n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = F.leaky_relu(self.dec1(z), 0.2)\n",
    "        x = F.leaky_relu(self.dec2(x), 0.2)\n",
    "        x = F.leaky_relu(self.dec3(x), 0.2)\n",
    "        x = F.leaky_relu(self.dec4(x), 0.2)\n",
    "        return (self.dec5(x))\n",
    "    \n",
    "    \n",
    "    def log_prob(self, x_hat, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        RE = F.mse_loss(x, x_hat, reduction='none').sum(1)\n",
    "        return RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowPrior(nn.Module):\n",
    "    def __init__(self, num_flows, D=50, M=256):\n",
    "        super(FlowPrior, self).__init__()\n",
    "        \n",
    "        nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "        nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "        self.D = D\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        (xa, xb) = torch.chunk(x, 2, 1)\n",
    "        xa, xb = xa.to(device), xb.to(device)\n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "\n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "\n",
    "        return torch.cat((xa, yb), 1), s\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "            log_det_J = log_det_J - s.sum(dim=1)\n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn(batch_size, self.D)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        z, log_det_J = self.f(x)\n",
    "        return log_standard_normal(z), log_det_J.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, D, M, variant='Normal', partial=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "        \n",
    "        self.encoder = Encoder(self.D, self.M, variant=self.variant, partial=self.partial)\n",
    "        self.decoder = Decoder(self.D, self.M)\n",
    "        if prior == 'Flow':\n",
    "            self.prior = FlowPrior(flows, D = M)\n",
    "        else:\n",
    "            self.prior = Prior(M)\n",
    "        \n",
    "            \n",
    "    def forward(self, x, reduction='avg'):\n",
    "        mu, log_var = self.encoder.forward(x)\n",
    "        z = self.encoder.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder.forward(z)\n",
    "\n",
    "        RE = self.decoder.log_prob(x_hat, x)\n",
    "        ENC = self.encoder.log_prob(z, mu, log_var)\n",
    "        if prior == 'Flow':\n",
    "            stdn, log_det_J = self.prior.log_prob(z)\n",
    "            return (RE, ENC, stdn, log_det_J)\n",
    "        else:\n",
    "            stdn = self.prior.log_prob(z)\n",
    "            return (RE, ENC, stdn)\n",
    "        \n",
    "        \n",
    "    def reconstruct(self, n=8):\n",
    "        for batch_idx, (x, _) in enumerate(valloader):\n",
    "            mu, log_std = self.encoder.forward(x[:n].to(device))\n",
    "            z = self.encoder.reparameterize(mu, log_std)\n",
    "            x_hat = self.decoder.forward(z)\n",
    "\n",
    "            save_image(torch.cat((x[:n].view(n, c, h, w), \n",
    "                                   x_hat.view(n, c, h, w).cpu())), \n",
    "                                 f\"{dataset}/fc/{variant+str(partial)}/{prior+str(flows)}/{subs}_reconstructed_{epoch}.png\", nrow=n)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    def generate(self, n=8):\n",
    "        z = self.prior.sample(n**2).to(device)\n",
    "        x_hat = model.decoder(z)\n",
    "        save_image(x_hat.view(n**2, c, h, w).cpu(), \n",
    "                   f\"{dataset}/fc/{variant+str(partial)}/{prior+str(flows)}/{subs}_generated_{epoch}.png\", nrow=n)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subs():\n",
    "    subs = 'F'\n",
    "    if variant == 'RandomNet' and partial == True:\n",
    "        subs += 'P'\n",
    "    elif variant == 'RandomNet' and partial == False:\n",
    "        subs += 'N'\n",
    "    else:\n",
    "        subs += 'L'\n",
    "\n",
    "    if prior == 'Flow':\n",
    "        subs += 'F'+str(flows)\n",
    "    else:\n",
    "        subs += 'N'\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "variant = 'RandomNet'\n",
    "partial = False\n",
    "mult = 2 if partial else 1\n",
    "prior = 'Flow'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "flows = 10\n",
    "n_epochs = 500\n",
    "lr = 0.001\n",
    "m=32\n",
    "\n",
    "subs = get_subs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, valloader, (c, h, w) = load_data(dataset=dataset, n_instances=n_instances, bs=bs)\n",
    "\n",
    "model = VAE(c*h*w, m, variant=variant, partial=partial)\n",
    "model.to(device)\n",
    "#model.apply(init_weights)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "n_weights = 0\n",
    "for parameter in model.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        n_weights += torch.tensor(parameter.shape).sum()\n",
    "\n",
    "print(f'There are {n_weights} weights in the VAE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses, RElosses, stdnlosses, KLlosses = [], [], [], []\n",
    "timer = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    t = time.time()\n",
    "    epochloss, REloss, stdnloss, KLloss = 0, 0, 0, 0\n",
    "    for batch_idx, (x, _) in enumerate(trainloader):\n",
    "        if prior == 'Flow':\n",
    "            RE, ENC, stdn, log_det_J = model.forward(x.to(device))\n",
    "            KL = (stdn + log_det_J - ENC).mean(-1)\n",
    "            loss = -(-RE + KL).mean()\n",
    "        else:\n",
    "            RE, ENC, stdn = model.forward(x.to(device))\n",
    "            KL =  (stdn - ENC).sum(-1)\n",
    "            loss = -(-RE + KL).mean()\n",
    "            \n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epochloss += loss.item()\n",
    "        REloss += RE.mean().item()\n",
    "        stdnloss += stdn.mean().item()\n",
    "        KLloss += KL.mean().item()\n",
    "        \n",
    "    losses.append(epochloss/len(trainloader))\n",
    "    RElosses.append(REloss/len(trainloader))\n",
    "    stdnlosses.append(stdnloss/len(trainloader))\n",
    "    KLlosses.append(KLloss/len(trainloader))\n",
    "    \n",
    "    print('[%d/%d]: loss: %.3f || RE: %.3f || KL: %.3f || stdn: %.3f' % ((epoch), n_epochs, epochloss/len(trainloader), \n",
    "                                                                         REloss/len(trainloader), KLloss/len(trainloader),\n",
    "                                                                         stdnloss/len(trainloader)))\n",
    "    timer.append(round(time.time() - t, 5))\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        model.reconstruct()\n",
    "        model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info={'Dataset': dataset,\n",
    "     'Variant': variant,\n",
    "     'Partial': partial,\n",
    "     'Prior': prior,\n",
    "     'n_instances': n_instances,\n",
    "     'batch_size': bs,\n",
    "     'flows': flows,\n",
    "     'n_epochs': n_epochs,\n",
    "     'learning_rate': lr,\n",
    "     'n_weights': n_weights,\n",
    "     'losses': list(np.asarray(losses).round(2)),\n",
    "     'RE': list(np.asarray(RElosses).round(2)),\n",
    "     'stdn': list(np.asarray(stdnlosses).round(2)),\n",
    "     'KL': list(np.asarray(KLlosses).round(2)),\n",
    "     'Times': list(np.asarray(timer).round(4))}\n",
    "\n",
    "pickle.dump(info, open(f'{dataset}/fc/{variant+str(partial)}/{prior+str(flows)}/info.p', 'wb'))\n",
    "torch.save(model, f'{dataset}/fc/{variant+str(partial)}/{prior+str(flows)}/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "n_epochs = 500\n",
    "lr = 0.001\n",
    "m = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subs():\n",
    "    subs = 'F'\n",
    "    if variant == 'RandomNet' and partial == True:\n",
    "        subs += 'P'\n",
    "    elif variant == 'RandomNet' and partial == False:\n",
    "        subs += 'N'\n",
    "    else:\n",
    "        subs += 'L'\n",
    "\n",
    "    if prior == 'Flow':\n",
    "        subs += 'F'+str(flows)\n",
    "    else:\n",
    "        subs += 'N'\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for variant, partial in [('Normal', True), ('RandomNet', True), ('RandomNet', False)]:\n",
    "    for prior, flows in [('Normal', 5), ('Flow', 5), ('Flow', 10)]:\n",
    "        model = torch.load(f'{dataset}/fc/{variant+str(partial)}/{prior+str(flows)}/model.pt')\n",
    "        subs = get_subs()\n",
    "        \n",
    "        m = torch.zeros((8, 8, 32))\n",
    "        m[0] = model.prior.sample(8)\n",
    "        m[7] = model.prior.sample(8)        \n",
    "              \n",
    "        for i in range(1, 7):\n",
    "            m[i] = m[0] - (((m[0] - m[7]) / 7)*i)\n",
    "            \n",
    "        for row in m.permute(1,0,2):\n",
    "            x_hat = model.decoder.forward(torch.Tensor(row).to(device)).reshape(8, 28, 28)\n",
    "            \n",
    "            if \"result_\" in dir():\n",
    "                result_ = torch.cat((result_, x_hat))\n",
    "            else:\n",
    "                result_ = x_hat \n",
    "                \n",
    "        save_image(result_.unsqueeze(1).detach().cpu(), \n",
    "                   f\"{subs}_interpolation.png\", nrow=8)\n",
    "        del result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
