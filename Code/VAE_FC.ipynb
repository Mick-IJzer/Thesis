{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROB FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "    \n",
    "def init_weights(m):\n",
    "    if (type(m) == nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST', bs=64, n_instances=5000):\n",
    "    if dataset == 'MNIST':\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Pad(2)])\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        trainset = transforms.Pad(2)(trainset.data).unsqueeze(1) / 255\n",
    "        \n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 60000 - n_instances])\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        c, h, w = 1, 32, 32\n",
    "        \n",
    "    else: #dataset == 'SVHN'\n",
    "        mat = scipy.io.loadmat('./data/train_32x32.mat')\n",
    "        trainset = torch.utils.data.TensorDataset((torch.Tensor(mat['X']) / 255).permute(3, 2, 0, 1),\n",
    "                                                 torch.Tensor(mat['y']))\n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 73257 - n_instances])\n",
    "        trainloader =  torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        c, h, w = 3, 32, 32\n",
    "        \n",
    "    return trainloader, valloader, (c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEF VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D, M, variant='Normal', partial=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "        self.std = torch.tensor(1e-3, requires_grad=False)\n",
    "        self.logvar = torch.log(torch.ones(bs, M, requires_grad=False)*(self.std**2)).to(device)\n",
    "        \n",
    "        if self.variant != 'RKS':\n",
    "            self.enc1 = nn.Linear(self.D, 256)\n",
    "            self.enc2 = nn.Linear(self.enc1.out_features, self.enc1.out_features//2)\n",
    "            self.enc3 = nn.Linear(self.enc2.out_features, self.enc2.out_features//2)\n",
    "            self.enc4 = nn.Linear(self.enc3.out_features, self.M*mult)\n",
    "\n",
    "            if self.variant != 'Normal':\n",
    "                self.enc1.weight.requires_grad = False\n",
    "                self.enc1.bias.requires_grad = False\n",
    "                self.enc2.weight.requires_grad = False\n",
    "                self.enc2.bias.requires_grad = False\n",
    "                self.enc3.weight.requires_grad = False\n",
    "                self.enc3.bias.requires_grad = False\n",
    "                if not self.partial:\n",
    "                    self.enc4.weight.requires_grad = False\n",
    "                    self.enc4.bias.requires_grad = False\n",
    "        \n",
    "        else:\n",
    "            self.enc1 = nn.Linear(self.D, self.M*mult)\n",
    "            self.enc1.weight.requires_grad = False\n",
    "            self.enc1.bias.requires_grad = False\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.variant != 'RKS':\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = F.leaky_relu(self.enc1(x), 0.2)\n",
    "            x = F.leaky_relu(self.enc2(x), 0.2)\n",
    "            x = F.leaky_relu(self.enc3(x), 0.2)\n",
    "            x = self.enc4(x)\n",
    "            if self.partial:\n",
    "                return torch.chunk(x, 2, dim=1)\n",
    "            else:\n",
    "                return x, None\n",
    "        else:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return self.enc1(x), None\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_std):\n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            eps = torch.randn_like(mu)\n",
    "            return mu + (eps*self.std)\n",
    "        else:\n",
    "            std = log_std.exp()\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + (eps*std)\n",
    "    \n",
    "    \n",
    "    def log_prob(self, z, mu, log_std):\n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            return log_normal_diag(z, mu, self.logvar[:z.shape[0]])\n",
    "        else:\n",
    "            return log_normal_diag(z, mu, log_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, D, M):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        \n",
    "        self.dec1 = nn.Linear(self.M, 128) \n",
    "        self.dec2 = nn.Linear(self.dec1.out_features, self.dec1.out_features*2)\n",
    "        self.dec3 = nn.Linear(self.dec2.out_features, self.dec2.out_features*2)\n",
    "        self.dec4 = nn.Linear(self.dec3.out_features, self.dec3.out_features*2)        \n",
    "        self.dec5 = nn.Linear(self.dec4.out_features, self.D)\n",
    "    \n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = F.leaky_relu(self.dec1(z), 0.2)\n",
    "        x = F.leaky_relu(self.dec2(x), 0.2)\n",
    "        x = F.leaky_relu(self.dec3(x), 0.2)\n",
    "        x = F.leaky_relu(self.dec4(x), 0.2)\n",
    "        return (self.dec5(x))\n",
    "    \n",
    "    \n",
    "    def log_prob(self, x_hat, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        RE = F.mse_loss(x, x_hat, reduction='none').sum(1)\n",
    "        return RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowPrior(nn.Module):\n",
    "    def __init__(self, num_flows, D=50, M=64):\n",
    "        super(FlowPrior, self).__init__()\n",
    "        \n",
    "        nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "        nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "        self.D = D\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        (xa, xb) = torch.chunk(x, 2, 1)\n",
    "        xa, xb = xa.to(device), xb.to(device)\n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "\n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "\n",
    "        return torch.cat((xa, yb), 1), s\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "            log_det_J = log_det_J - s.sum(dim=1)\n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn(batch_size, self.D)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, self.D)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        z, log_det_J = self.f(x)\n",
    "        return log_standard_normal(z) + log_det_J.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, c, D, M, variant='Normal', partial=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.c = c\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "        \n",
    "        self.encoder = Encoder(self.D, self.M, variant=self.variant, partial=self.partial)\n",
    "        self.decoder = Decoder(self.D, self.M)\n",
    "        if prior == 'Flow':\n",
    "            self.prior = FlowPrior(flows, D = M)\n",
    "        else:\n",
    "            self.prior = Prior(M)\n",
    "        \n",
    "            \n",
    "    def forward(self, x, reduction='avg'):\n",
    "        mu, log_var = self.encoder.forward(x)\n",
    "        z = self.encoder.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder.forward(z)\n",
    "\n",
    "        RE = self.decoder.log_prob(x_hat, x)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(z, mu, log_var)).mean(-1)\n",
    "        return RE, KL\n",
    "        \n",
    "        \n",
    "    def reconstruct(self, n=8):\n",
    "        for batch_idx, (x, _) in enumerate(valloader):\n",
    "            mu, log_std = self.encoder.forward(x[:n].to(device))\n",
    "            z = self.encoder.reparameterize(mu, log_std)\n",
    "            x_hat = self.decoder.forward(z)\n",
    "\n",
    "            save_image(torch.cat((x[:n].view(n, c, h, w), \n",
    "                                   x_hat.view(n, c, h, w).cpu())), \n",
    "                                 f\"{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/{subs}_reconstructed_{epoch}.png\", nrow=n)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    def generate(self, n=8):\n",
    "        z = self.prior.sample(n**2).to(device)\n",
    "        x_hat = self.decoder(z)\n",
    "        save_image(x_hat.view(n**2, c, h, w).cpu(), \n",
    "                   f\"{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/{subs}_generated_{epoch}.png\", nrow=n)\n",
    "        \n",
    "    \n",
    "    def interpolate(self):\n",
    "        m = torch.zeros((8, 8, M))\n",
    "        m[0] = self.prior.sample(8)\n",
    "        m[7] = self.prior.sample(8)        \n",
    "              \n",
    "        for i in range(1, 7):\n",
    "            m[i] = m[0] - (((m[0] - m[7]) / 7)*i)\n",
    "            \n",
    "        for row in m.permute(1,0,2):\n",
    "            x_hat = self.decoder.forward(torch.Tensor(row).to(device)).reshape(8, self.c, 32, 32)\n",
    "            \n",
    "            if \"result_\" in dir():\n",
    "                result_ = torch.cat((result_, x_hat))\n",
    "            else:\n",
    "                result_ = x_hat \n",
    "                \n",
    "        save_image(result_.detach().cpu(), \n",
    "                   f\"{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/{subs}_interpolation_{epoch}.png\", nrow=8)\n",
    "        del result_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subs():\n",
    "    subs = 'F'\n",
    "    if variant == 'RandomNet' and partial == True:\n",
    "        subs += 'P'\n",
    "    elif variant == 'RandomNet' and partial == False:\n",
    "        subs += 'N'\n",
    "    elif variant == 'RKS':\n",
    "        subs += 'R'\n",
    "    else:\n",
    "        subs += 'L'\n",
    "\n",
    "    if prior == 'Flow':\n",
    "        subs += 'F'+str(flows)\n",
    "    else:\n",
    "        subs += 'N'\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_times():\n",
    "    model = VAE(c, D, M, variant=variant, partial=partial)\n",
    "    model.to(device)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "    n_weights = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            n_weights += torch.tensor(parameter.shape).sum()\n",
    "\n",
    "    print(f'There are {n_weights} weights in the VAE.')\n",
    "\n",
    "\n",
    "    losses, RElosses, KLlosses = [], [], []\n",
    "    timer = []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t = time.time()\n",
    "        epochloss, REloss, stdnloss, KLloss = 0, 0, 0, 0\n",
    "        for batch_idx, x in enumerate(trainloader):\n",
    "            RE, KL = model.forward(x.to(device))\n",
    "            loss = -(-RE + KL).mean()\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            epochloss += loss.item()\n",
    "            REloss += RE.mean().item()\n",
    "            KLloss += KL.mean().item()\n",
    "\n",
    "        losses.append(epochloss/len(trainloader))\n",
    "        RElosses.append(REloss/len(trainloader))\n",
    "        KLlosses.append(KLloss/len(trainloader))\n",
    "\n",
    "        print('[%d/%d]: loss: %.3f || RE: %.3f || KL: %.3f' % ((epoch), n_epochs, epochloss/len(trainloader), \n",
    "                                                                                 REloss/len(trainloader), KLloss/len(trainloader)))\n",
    "        timer.append(round(time.time() - t, 5))\n",
    "        \n",
    "    return timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    global epoch\n",
    "    model = VAE(c, D, M, variant=variant, partial=partial)\n",
    "    model.to(device)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "    n_weights = 0\n",
    "    for parameter in model.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            n_weights += torch.tensor(parameter.shape).sum()\n",
    "\n",
    "    print(f'There are {n_weights} weights in the VAE.')\n",
    "    \n",
    "    \n",
    "    losses, RElosses, KLlosses = [], [], []\n",
    "    timer = []\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t = time.time()\n",
    "        epochloss, REloss, stdnloss, KLloss = 0, 0, 0, 0\n",
    "        for batch_idx, (x, _) in enumerate(trainloader):\n",
    "            RE, KL = model.forward(x.to(device))\n",
    "            loss = -(-RE + KL).mean()\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            epochloss += loss.item()\n",
    "            REloss += RE.mean().item()\n",
    "            KLloss += KL.mean().item()\n",
    "\n",
    "        losses.append(epochloss/len(trainloader))\n",
    "        RElosses.append(REloss/len(trainloader))\n",
    "        KLlosses.append(KLloss/len(trainloader))\n",
    "\n",
    "        print('[%d/%d]: loss: %.3f || RE: %.3f || KL: %.3f ' % ((epoch), n_epochs, epochloss/len(trainloader), \n",
    "                                                                             REloss/len(trainloader), KLloss/len(trainloader)))\n",
    "        timer.append(round(time.time() - t, 5))\n",
    "\n",
    "        if epoch%10 == 0:\n",
    "            model.reconstruct()\n",
    "            model.generate()\n",
    "            model.interpolate()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss, test_re, test_kl = 0, 0, 0\n",
    "        \n",
    "        for batch, _ in valloader:\n",
    "            RE, KL = model.forward(batch.to(device))\n",
    "            \n",
    "            loss = -(-RE + KL).mean()\n",
    "            test_loss += (loss.item() / len(valloader))\n",
    "            test_re += (RE.mean().item() / len(valloader))\n",
    "            test_kl += (KL.mean().item() / len(valloader))\n",
    "    \n",
    "    print('TEST: loss: %.3f || RE: %.3f || KL: %.3f ' % (test_loss, test_re, test_kl))\n",
    "            \n",
    "    info={'Dataset': dataset,\n",
    "         'Variant': variant,\n",
    "         'Partial': partial,\n",
    "         'Prior': prior,\n",
    "         'n_instances': n_instances,\n",
    "         'batch_size': bs,\n",
    "         'flows': flows,\n",
    "         'n_epochs': n_epochs,\n",
    "         'learning_rate': lr,\n",
    "         'n_weights': n_weights,\n",
    "         'losses': list(np.asarray(losses).round(4)),\n",
    "         'RE': list(np.asarray(RElosses).round(4)),\n",
    "         'KL': list(np.asarray(KLlosses).round(4)),\n",
    "         'Times': list(np.asarray(timer).round(4)),\n",
    "         'Test_loss': round(test_loss, 4),\n",
    "         'Test_RE': round(test_re, 4),\n",
    "         'Test_KL': round(test_kl, 4)}\n",
    "\n",
    "    pickle.dump(info, open(f'{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/info.p', 'wb'))\n",
    "    torch.save(model, f'{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "n_epochs = 250\n",
    "lr = 0.001\n",
    "M = 32 if dataset == 'SVHN' else 16\n",
    "\n",
    "for REPEAT in range(2, 6):\n",
    "    print('ITERATION', REPEAT)\n",
    "    trainloader, valloader, (c, h, w) = load_data(dataset=dataset, n_instances=n_instances, bs=bs)\n",
    "    D=c*h*w\n",
    "   \n",
    "    for (variant, partial) in [('RandomNet', False), ('RandomNet', True), ('RKS', False), ('Normal', True)]:\n",
    "        for (prior, flows) in [('Flow', 6), ('Flow', 2), ('Normal', 2)]:\n",
    "\n",
    "            mult = 2 if partial else 1 \n",
    "            subs = get_subs()\n",
    "            print(subs)\n",
    "\n",
    "            experiment()\n",
    "            print()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FID-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_fid import fid_score\n",
    "from pytorch_fid.inception import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 2048\n",
    "\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "inception = InceptionV3([block_idx]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "n_epochs = 250\n",
    "lr = 0.001\n",
    "M = 8 if dataset == 'SVHN' else 4\n",
    "n = 10000\n",
    "\n",
    "trainloader, valloader, (c, h, w) = load_data(dataset=dataset, n_instances=n_instances, bs=n)\n",
    "    \n",
    "for batch in trainloader:\n",
    "    break\n",
    "            \n",
    "for i, x in enumerate(batch):\n",
    "    save_image(x, f\"FID/Original/sample{i}.png\")\n",
    "\n",
    "og_m, og_s = fid_score.compute_statistics_of_path(\"FID/Original\", model=inception, batch_size=100, \n",
    "                                                  dims=2048, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for REPEAT in range(1, 6):\n",
    "    print('ITERATION', REPEAT)\n",
    "\n",
    "    for (variant, partial) in [('RandomNet', False), ('RandomNet', True), ('RKS', False), ('Normal', True)]:\n",
    "        for (prior, flows) in [('Flow', 6), ('Flow', 2), ('Normal', 2)]:\n",
    "            model = torch.load(f'{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/model.pt')\n",
    "            info = pickle.load(open(f'{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/info.p', 'rb'))\n",
    "            subs=get_subs()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                z = model.prior.sample(n).to(device)\n",
    "                x_hat = model.decoder(z)\n",
    "\n",
    "            for i, x in enumerate(x_hat):\n",
    "                save_image(x.reshape(32, 32), f\"FID/Generated/sample{i}.png\")\n",
    "            \n",
    "            gen_m, gen_s = fid_score.compute_statistics_of_path(\"FID/Generated\", model=inception, batch_size=100, \n",
    "                                                                dims=2048, device=device)\n",
    "            fid = fid_score.calculate_frechet_distance(og_m, og_s, gen_m, gen_s)\n",
    "            print(f'{subs}: {round(fid, 4)}')\n",
    "            \n",
    "            info['FID_score'] = fid\n",
    "            pickle.dump(info, open(f'{dataset}-{str(REPEAT)}/fc/{variant+str(partial)}/{prior+str(flows)}/info.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
