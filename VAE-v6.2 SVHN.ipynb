{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tarfile \n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = torch.from_numpy(np.asarray(np.pi))\n",
    "EPS = 1.e-5\n",
    "\n",
    "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
    "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
    "\n",
    "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_bernoulli(x, p, reduction=None, dim=None):\n",
    "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
    "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p\n",
    "\n",
    "def log_standard_normal(x, reduction=None, dim=None):\n",
    "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.\n",
    "    if reduction == 'avg':\n",
    "        return torch.mean(log_p, dim)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(log_p, dim)\n",
    "    else:\n",
    "        return log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST', bs=64, n_instances=5000):\n",
    "    if dataset == 'MNIST':\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 60000 - n_instances])\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        c, h, w = 1, 28, 28\n",
    "        \n",
    "    else: #dataset == 'SVHN'\n",
    "        mat = scipy.io.loadmat('./data/train_32x32.mat')\n",
    "        trainset = torch.utils.data.TensorDataset((torch.Tensor(mat['X']) / 255).permute(3, 2, 0, 1),\n",
    "                                                 torch.Tensor(mat['y']))\n",
    "        trainset, valset = torch.utils.data.random_split(trainset, [n_instances, 73257 - n_instances])\n",
    "        trainloader =  torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "        valloader =  torch.utils.data.DataLoader(valset, batch_size=bs, shuffle=True)\n",
    "        \n",
    "        c, h, w = 3, 32, 32\n",
    "        \n",
    "    return trainloader, valloader, (c, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, c, h, w, variant='Normal', partial=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.c, self.h, self.w, self.D = c, h, w, c*h*w\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "        self.variance = torch.tensor(1e-10)\n",
    "        \n",
    "        if variant != 'RKS':\n",
    "            self.enc1 = nn.Conv2d(self.c, 4*mult, kernel_size=3, stride=1, padding=1)\n",
    "            self.enc2 = nn.Conv2d(4*mult, 8*mult, kernel_size=3, stride=1, padding=1)\n",
    "            self.enc3 = nn.Conv2d(8*mult, 16*mult, kernel_size=3, stride=1, padding=1)\n",
    "            self.enc4 = nn.Conv2d(16*mult, 32*mult, kernel_size=3, stride=1)\n",
    "\n",
    "            if self.variant != 'Normal':\n",
    "                self.enc1.weight.requires_grad = False\n",
    "                self.enc1.bias.requires_grad = False\n",
    "                self.enc2.weight.requires_grad = False\n",
    "                self.enc2.bias.requires_grad = False\n",
    "                self.enc3.weight.requires_grad = False\n",
    "                self.enc3.bias.requires_grad = False\n",
    "                if not self.partial:\n",
    "                    self.enc4.weight.requires_grad = False\n",
    "                    self.enc4.bias.requires_grad = False\n",
    "        else:\n",
    "            #self.enc1 = nn.Linear(32*32*3, 32*2*2)\n",
    "            self.enc1 = nn.Conv2d(self.c, 32, kernel_size=16, stride=16)\n",
    "            self.enc1.weight.requires_grad = False\n",
    "            self.enc1.bias.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.variant != 'RKS':\n",
    "            x = F.max_pool2d(F.relu(self.enc1(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.enc2(x)), 2)\n",
    "            x = F.max_pool2d(F.relu(self.enc3(x)), 2)\n",
    "            x = self.enc4(x)\n",
    "            return torch.chunk(x, 2, dim=1)\n",
    "        else:\n",
    "            #x = x.view(-1, 32*32*3)\n",
    "            x = self.enc1(x)\n",
    "            #x = x.view(-1, 32, 2, 2)\n",
    "            return torch.chunk(x, 2, dim=1)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            mu = torch.cat((mu, log_var), dim=1)\n",
    "            std = torch.sqrt(self.variance)\n",
    "            eps = torch.randn_like(mu)\n",
    "            return mu + (eps*std)\n",
    "        else:\n",
    "            std = torch.sqrt(log_var.exp())\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + (eps*std)\n",
    "    \n",
    "    \n",
    "    def log_prob(self, z, mu, log_var):    \n",
    "        if (not self.partial) and (self.variant != 'Normal'):\n",
    "            mu = torch.cat((mu, log_var), dim=1)\n",
    "            z, mu = z.view(-1, 32*2*2), mu.view(-1, 32*2*2)\n",
    "            return log_normal_diag(z, mu, (torch.ones(z.shape)*torch.log(self.variance)).to(device))\n",
    "        else:\n",
    "            z, mu, log_var = z.view(-1, 32*2*2), mu.view(-1, 32*2*2), log_var.view(-1, 32*2*2)\n",
    "            return log_normal_diag(z, mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, c, h, w):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.c, self.h, self.w, self.D = c, h, w, c*h*w\n",
    "        \n",
    "        self.dec1 = nn.Conv2d(32, 24, 3, padding=1)\n",
    "        self.dec2 = nn.Conv2d(24, 16, 3, padding=1)\n",
    "        self.dec3 = nn.Conv2d(16, 12, 3, padding=1)\n",
    "        self.dec4 = nn.Conv2d(12, 8, 3, padding=1)\n",
    "        self.dec5 = nn.Conv2d(8, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.upsample(F.leaky_relu(self.dec1(z), 0.2), scale_factor=2)\n",
    "        x = F.upsample(F.leaky_relu(self.dec2(x), 0.2), scale_factor=2)\n",
    "        x = F.upsample(F.leaky_relu(self.dec3(x), 0.2), scale_factor=2)\n",
    "        x = F.upsample(F.leaky_relu(self.dec4(x), 0.2), scale_factor=2)\n",
    "        x = self.dec5(x)\n",
    "        return F.interpolate(x, size=(32, 32), mode='nearest')\n",
    "    \n",
    "    \n",
    "    def log_prob(self, x_hat, x):\n",
    "        RE = F.mse_loss(x_hat, x, reduction='none').sum(dim=(1,2,3))\n",
    "        return RE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Prior, self).__init__()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn((batch_size, 32, 2, 2))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        return log_standard_normal(z.view(-1, 32*2*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowPrior(nn.Module):\n",
    "    def __init__(self, num_flows, D=50, M=256):\n",
    "        super(FlowPrior, self).__init__()\n",
    "        \n",
    "        nets = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2), nn.Tanh())\n",
    "\n",
    "        nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                             nn.Linear(M, D // 2))\n",
    "\n",
    "        self.D = D\n",
    "        self.t = torch.nn.ModuleList([nett() for _ in range(num_flows)])\n",
    "        self.s = torch.nn.ModuleList([nets() for _ in range(num_flows)])\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        \n",
    "    def coupling(self, x, index, forward=True):\n",
    "        (xa, xb) = torch.chunk(x, 2, 1)\n",
    "        xa, xb = xa.to(device), xb.to(device)\n",
    "        s = self.s[index](xa)\n",
    "        t = self.t[index](xa)\n",
    "\n",
    "        if forward:\n",
    "            #yb = f^{-1}(x)\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            #xb = f(y)\n",
    "            yb = torch.exp(s) * xb + t\n",
    "\n",
    "        return torch.cat((xa, yb), 1), s\n",
    "\n",
    "    \n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        log_det_J, z = x.new_zeros(x.shape[0]), x\n",
    "        for i in range(self.num_flows):\n",
    "            z, s = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "            log_det_J = log_det_J - s.sum(1)\n",
    "\n",
    "        return z, log_det_J\n",
    "\n",
    "    \n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x, _ = self.coupling(x, i, forward=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn(batch_size, self.D)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(-1, 32, 2, 2)\n",
    "\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        z, log_det_J = self.f(x.view(-1, 32*2*2))\n",
    "        return log_standard_normal(z), log_det_J.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, c, h, w, variant='Normal', partial=True):\n",
    "        super(VAE, self).__init__()\n",
    "        self.variant = variant\n",
    "        self.partial = partial\n",
    "            \n",
    "        self.encoder = Encoder(c, h, w, variant=self.variant, partial=self.partial)\n",
    "        self.decoder = Decoder(c, h, w)\n",
    "        if prior == 'Flow':\n",
    "            self.prior = FlowPrior(flows, D = 32*2*2)\n",
    "        else:\n",
    "            self.prior = Prior()\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder.forward(x)\n",
    "        z = self.encoder.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder.forward(z)\n",
    "\n",
    "        RE = self.decoder.log_prob(x_hat, x)\n",
    "        ENC = self.encoder.log_prob(z, mu, log_var)\n",
    "        if prior == 'Flow':\n",
    "            stdn, log_det_J = self.prior.log_prob(z)\n",
    "            return (RE, ENC, stdn, log_det_J)\n",
    "        else:\n",
    "            stdn = self.prior.log_prob(z)\n",
    "            return (RE, ENC, stdn)\n",
    "        \n",
    "        \n",
    "    def reconstruct(self, n=8):\n",
    "        for batch_idx, (x, _) in enumerate(valloader):\n",
    "            mu, log_var = self.encoder.forward(x[:n].to(device))\n",
    "            z = self.encoder.reparameterize(mu, log_var)\n",
    "            x_hat = self.decoder.forward(z)\n",
    "\n",
    "            save_image(torch.cat((x[:n].view(n, c, h, w), \n",
    "                                   x_hat.view(n, c, h, w).cpu())), \n",
    "                                 f\"{dataset}/conv/{variant+str(partial)}/{prior+str(flows)}/{subs}_reconstructed_{epoch}.png\", nrow=n)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    def generate(self, n=8):\n",
    "        z = self.prior.sample(n**2).to(device)\n",
    "        x_hat = model.decoder(z)\n",
    "        save_image(x_hat.view(n**2, c, h, w).cpu(), \n",
    "                   f\"{dataset}/conv/{variant+str(partial)}/{prior+str(flows)}/{subs}_generated_{epoch}.png\", nrow=n)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize + Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'SVHN'\n",
    "variant = 'RKS' #'RandomNet'\n",
    "partial = False\n",
    "mult = 2 if partial else 1\n",
    "prior = 'Flow' #'Flow'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "flows = 10\n",
    "n_epochs = 500\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "subs = 'C'\n",
    "if variant == 'RandomNet' and partial == True:\n",
    "    subs += 'P'\n",
    "elif variant == 'RandomNet' and partial == False:\n",
    "    subs += 'N'\n",
    "elif variant == 'RKS' and partial == False:\n",
    "    subs += 'R'\n",
    "else:\n",
    "    subs += 'L'\n",
    "    \n",
    "if prior == 'Flow':\n",
    "    subs += 'F'+str(flows)\n",
    "else:\n",
    "    subs += 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34808 weights in the VAE.\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader, (c, h, w) = load_data(dataset=dataset, n_instances=n_instances, bs=bs)\n",
    "\n",
    "model = VAE(c, h, w, variant=variant, partial=partial)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = lr)\n",
    "\n",
    "n_weights = 0\n",
    "for parameter in model.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        n_weights += torch.tensor(parameter.shape).sum()\n",
    "\n",
    "print(f'There are {n_weights} weights in the VAE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (enc1): Conv2d(3, 32, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dec1): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (dec2): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (dec3): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (dec4): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (dec5): Conv2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (prior): FlowPrior(\n",
       "    (t): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (s): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "        (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m, distr = 'Xavier', uniform=True):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        if distr == 'Kaiming' and uniform:\n",
    "            torch.nn.init.kaiming_uniform(m.weight)\n",
    "        elif distr == 'Kaiming' and not uniform:\n",
    "            torch.nn.init.kaiming_normal(m.weight)\n",
    "        elif distr == 'Xavier' and uniform:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "        else:  # distr == 'Xavier' and not uniform:\n",
    "            torch.nn.init.xavier_normal(m.weight)\n",
    "        \n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500]: loss: -384.289 || RE: 141.411 || KL: 525.700 || stdn: -25.556\n",
      "[2/500]: loss: -569.895 || RE: 42.597 || KL: 612.492 || stdn: -14.821\n",
      "[3/500]: loss: -585.089 || RE: 31.204 || KL: 616.293 || stdn: -11.544\n",
      "[4/500]: loss: -590.544 || RE: 26.925 || KL: 617.468 || stdn: -10.409\n",
      "[5/500]: loss: -593.730 || RE: 24.392 || KL: 618.122 || stdn: -9.772\n",
      "[6/500]: loss: -595.942 || RE: 22.603 || KL: 618.545 || stdn: -9.356\n",
      "[7/500]: loss: -597.382 || RE: 21.372 || KL: 618.754 || stdn: -9.614\n",
      "[8/500]: loss: -600.210 || RE: 20.400 || KL: 620.610 || stdn: -9.292\n",
      "[9/500]: loss: -601.371 || RE: 19.559 || KL: 620.930 || stdn: -8.974\n",
      "[10/500]: loss: -602.127 || RE: 18.974 || KL: 621.100 || stdn: -8.804\n",
      "[11/500]: loss: -602.976 || RE: 18.244 || KL: 621.220 || stdn: -8.684\n",
      "[12/500]: loss: -603.466 || RE: 17.881 || KL: 621.347 || stdn: -8.557\n",
      "[13/500]: loss: -604.190 || RE: 17.219 || KL: 621.409 || stdn: -8.496\n",
      "[14/500]: loss: -604.776 || RE: 16.767 || KL: 621.544 || stdn: -8.362\n",
      "[15/500]: loss: -605.190 || RE: 16.422 || KL: 621.612 || stdn: -8.293\n",
      "[16/500]: loss: -605.767 || RE: 15.967 || KL: 621.735 || stdn: -8.170\n",
      "[17/500]: loss: -606.100 || RE: 15.713 || KL: 621.813 || stdn: -8.093\n",
      "[18/500]: loss: -606.448 || RE: 15.440 || KL: 621.888 || stdn: -8.019\n",
      "[19/500]: loss: -606.777 || RE: 15.230 || KL: 622.007 || stdn: -7.898\n",
      "[20/500]: loss: -607.226 || RE: 14.836 || KL: 622.062 || stdn: -7.844\n",
      "[21/500]: loss: -607.479 || RE: 14.670 || KL: 622.150 || stdn: -7.756\n",
      "[22/500]: loss: -607.675 || RE: 14.530 || KL: 622.205 || stdn: -7.701\n",
      "[23/500]: loss: -608.018 || RE: 14.261 || KL: 622.279 || stdn: -7.627\n",
      "[24/500]: loss: -608.196 || RE: 14.132 || KL: 622.328 || stdn: -7.577\n",
      "[25/500]: loss: -608.570 || RE: 13.839 || KL: 622.409 || stdn: -7.497\n",
      "[26/500]: loss: -608.793 || RE: 13.681 || KL: 622.474 || stdn: -7.432\n",
      "[27/500]: loss: -608.705 || RE: 13.829 || KL: 622.534 || stdn: -7.372\n",
      "[28/500]: loss: -609.196 || RE: 13.393 || KL: 622.589 || stdn: -7.317\n",
      "[29/500]: loss: -609.442 || RE: 13.226 || KL: 622.668 || stdn: -7.238\n",
      "[30/500]: loss: -609.418 || RE: 13.270 || KL: 622.688 || stdn: -7.218\n",
      "[31/500]: loss: -609.695 || RE: 13.043 || KL: 622.738 || stdn: -7.168\n",
      "[32/500]: loss: -609.881 || RE: 12.914 || KL: 622.795 || stdn: -7.111\n",
      "[33/500]: loss: -609.984 || RE: 12.887 || KL: 622.871 || stdn: -7.036\n",
      "[34/500]: loss: -610.218 || RE: 12.677 || KL: 622.895 || stdn: -7.011\n",
      "[35/500]: loss: -610.345 || RE: 12.617 || KL: 622.962 || stdn: -6.943\n",
      "[36/500]: loss: -610.448 || RE: 12.551 || KL: 622.999 || stdn: -6.907\n",
      "[37/500]: loss: -610.607 || RE: 12.434 || KL: 623.041 || stdn: -6.865\n",
      "[38/500]: loss: -610.739 || RE: 12.330 || KL: 623.069 || stdn: -6.837\n",
      "[39/500]: loss: -610.867 || RE: 12.239 || KL: 623.106 || stdn: -6.800\n",
      "[40/500]: loss: -610.943 || RE: 12.213 || KL: 623.156 || stdn: -6.751\n",
      "[41/500]: loss: -611.137 || RE: 12.054 || KL: 623.191 || stdn: -6.714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-21c6ef7a5249>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprior\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Flow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mRE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_det_J\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mKL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlog_det_J\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mENC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mRE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mKL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-cc1824f64923>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mENC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprior\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Flow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mstdn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_det_J\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mRE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_det_J\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-9e98a3121774>\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_det_J\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_standard_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_det_J\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-9e98a3121774>\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mlog_det_J\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_flows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoupling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mlog_det_J\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_det_J\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-9e98a3121774>\u001b[0m in \u001b[0;36mcoupling\u001b[1;34m(self, x, index, forward)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, RElosses, stdnlosses, KLlosses = [], [], [], []\n",
    "timer = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    t = time.time()\n",
    "    epochloss, REloss, stdnloss, KLloss = 0, 0, 0, 0\n",
    "    for batch_idx, (x, _) in enumerate(trainloader):\n",
    "        if prior == 'Flow':\n",
    "            RE, ENC, stdn, log_det_J = model.forward(x.to(device))\n",
    "            KL = (stdn + log_det_J - ENC).mean(-1)\n",
    "            loss = -(-RE + KL).mean()\n",
    "        else:\n",
    "            RE, ENC, stdn = model.forward(x.to(device))\n",
    "            KL =  (stdn - ENC).sum(-1)\n",
    "            loss = -(-RE + KL).mean()\n",
    "            \n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epochloss += loss.item()\n",
    "        REloss += RE.mean().item()\n",
    "        stdnloss += stdn.mean().item()\n",
    "        KLloss += KL.mean().item()\n",
    "        \n",
    "    losses.append(epochloss/len(trainloader))\n",
    "    RElosses.append(REloss/len(trainloader))\n",
    "    stdnlosses.append(stdnloss/len(trainloader))\n",
    "    KLlosses.append(KLloss/len(trainloader))\n",
    "    \n",
    "    print('[%d/%d]: loss: %.3f || RE: %.3f || KL: %.3f || stdn: %.3f' % ((epoch), n_epochs, epochloss/len(trainloader), \n",
    "                                                                         REloss/len(trainloader), KLloss/len(trainloader),\n",
    "                                                                         stdnloss/len(trainloader)))\n",
    "    timer.append(round(time.time() - t, 5))\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        model.reconstruct()\n",
    "        model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info={'Dataset': dataset,\n",
    "     'Variant': variant,\n",
    "     'Partial': partial,\n",
    "     'Prior': prior,\n",
    "     'n_instances': n_instances,\n",
    "     'batch_size': bs,\n",
    "     'flows': flows,\n",
    "     'n_epochs': n_epochs,\n",
    "     'learning_rate': lr,\n",
    "     'n_weights': n_weights,\n",
    "     'losses': list(np.asarray(losses).round(2)),\n",
    "     'RE': list(np.asarray(RElosses).round(2)),\n",
    "     'stdn': list(np.asarray(stdnlosses).round(2)),\n",
    "     'KL': list(np.asarray(KLlosses).round(2)),\n",
    "     'Times': list(np.asarray(timer).round(4))}\n",
    "\n",
    "pickle.dump(info, open(f'{dataset}/conv/{variant+str(partial)}/{prior+str(flows)}/info.p', 'wb'))\n",
    "torch.save(model, f'{dataset}/conv/{variant+str(partial)}/{prior+str(flows)}/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'SVHN'\n",
    "n_instances = 25000\n",
    "bs = 256\n",
    "n_epochs = 500\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subs():\n",
    "    subs = 'C'\n",
    "    if variant == 'RandomNet' and partial == True:\n",
    "        subs += 'P'\n",
    "    elif variant == 'RandomNet' and partial == False:\n",
    "        subs += 'N'\n",
    "    elif variant == 'RKS' and partial == False:\n",
    "        subs += 'R'\n",
    "    else:\n",
    "        subs += 'L'\n",
    "\n",
    "    if prior == 'Flow':\n",
    "        subs += 'F'+str(flows)\n",
    "    else:\n",
    "        subs += 'N'\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant, partial in [('Normal', True), ('RandomNet', True), ('RandomNet', False), ('RKS', False)]:\n",
    "    for prior, flows in [('Flow', 10)]:\n",
    "        model = torch.load(f'{dataset}/conv/{variant+str(partial)}/{prior+str(flows)}/model.pt')\n",
    "        subs = get_subs()\n",
    "        \n",
    "        m = torch.zeros((8, 8, 32, 2, 2))\n",
    "        m[0] = model.prior.sample(8)\n",
    "        m[7] = model.prior.sample(8)\n",
    "              \n",
    "        for i in range(1, 7):\n",
    "            m[i] = m[0] - (((m[0] - m[7]) / 7)*i)\n",
    "            \n",
    "        for row in m.permute(1,0,2,3,4):\n",
    "            x_hat = model.decoder.forward(torch.Tensor(row).to(device))\n",
    "            \n",
    "            if \"result_\" in dir():\n",
    "                result_ = torch.cat((result_, x_hat))\n",
    "            else:\n",
    "                result_ = x_hat \n",
    "                \n",
    "        save_image(result_.detach().cpu(), \n",
    "                   f\"SVHN_{subs}_interpolation.png\", nrow=8)\n",
    "        del result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
